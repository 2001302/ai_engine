#include <iostream>
#include <filesystem>
#include "ggml.h"
#include "llama.h"
#include "llama.cpp\common\grammar-parser.h"

int main() {

    //const std::string model_path = "llama-2-7b-chat.Q2_K.gguf";

    //if (std::filesystem::exists(model_path)) {
    //    std::cout << "exist file" << std::endl;
    //}
    //
    //llama_backend_init();

    //llama_model_params model_params = llama_model_default_params();
    ////model_params.n_gpu_layers = llamafile_gpu_layers(35);
    //llama_model* model = llama_load_model_from_file(model_path.c_str(), model_params);


    return 0;
}
